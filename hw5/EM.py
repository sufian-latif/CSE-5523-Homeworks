#!/usr/bin/python

#########################################################
# CSE 5523 starter code (HW#5)
# Alan Ritter
#########################################################

import random
import math
import sys
import re

# GLOBALS/Constants
VAR_INIT = 1

def logExpSum(x):
    # TODO: implement logExpSum
    pass

def readTrue(filename='wine-true.data'):
    f = open(filename)
    labels = []
    splitRe = re.compile(r"\s")
    for line in f:
        labels.append(int(splitRe.split(line)[0]))
    return labels

#########################################################################
# Reads and manages data in appropriate format
#########################################################################
class Data:
    def __init__(self, filename):
        self.data = []
        f = open(filename)
        (self.nRows, self.nCols) = [int(x) for x in f.readline().split(" ")]
        for line in f:
            self.data.append([float(x) for x in line.split(" ")])

    # Computers the range of each column (returns a list of min-max tuples)
    def Range(self):
        ranges = []
        for j in range(self.nCols):
            min = self.data[0][j]
            max = self.data[0][j]
            for i in range(1, self.nRows):
                if self.data[i][j] > max:
                    max = self.data[i][j]
                if self.data[i][j] < min:
                    min = self.data[i][j]
            ranges.append((min, max))
        return ranges

    def __getitem__(self, row):
        return self.data[row]

#########################################################################
# Computes EM on a given data set, using the specified number of clusters
# self.parameters is a tuple containing the mean and variance for each gaussian
#########################################################################
class EM:
    def __init__(self, data, nClusters):
        # Initialize parameters randomly...
        random.seed()
        self.parameters = []
        self.priors = []        # Cluster priors
        self.nClusters = nClusters
        self.data = data
        ranges = data.Range()
        for i in range(nClusters):
            p = []
            initRow = random.randint(0, data.nRows-1)
            for j in range(data.nCols):
                # Randomly initalize variance in range of data
                p.append((random.uniform(ranges[j][0], ranges[j][1]), VAR_INIT*(ranges[j][1] - ranges[j][0])))
            self.parameters.append(p)

        # Initialize priors uniformly
        for c in range(nClusters):
            self.priors.append(1/float(nClusters))

    def LogLikelihood(self, data):
        logLikelihood = 0.0
        # TODO: compute log-likelihood of the data
        return logLikelihood

    # Compute marginal distributions of hidden variables
    def Estep(self):
        # TODO: E-step
        pass

    # Update the parameter estimates
    def Mstep(self):
        # TODO: M-step
        pass

    # Computes the probability that row was generated by cluster
    def LogProb(self, row, cluster, data):
        # TODO: compute probability row i was generated by cluster k
        prior = self.priors[cluster]
        param = self.parameters[cluster]
        x = data[row]
        a = 0
        b = 0
        M = data.nCols
        for i in range(M):
            mean, var = param[i]
            a += var ** 2
            b += (x[i] - mean) ** 2 / var ** 2

        return -0.5 * M * math.log(2 * math.pi) - 0.5 * math.log(a) - 0.5 * b

    def Run(self, maxsteps=100, testData=None):
        # TODO: Implement EM algorithm
        trainLikelihood = 0.0
        testLikelihood = 0.0
        return (trainLikelihood, testLikelihood)

if __name__ == "__main__":
    d = Data('wine.train')
    if len(sys.argv) > 1:
        e = EM(d, int(sys.argv[1]))
        for i in range(d.nRows):
            print e.LogProb(i, 0, d)
    else:
        e = EM(d, 3)
    e.Run(100)
